{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='career'></a>\n",
    "<a href=\"#\"><img src=\"/_static/img/jupyter-notebook-download-blue.svg\" id=\"notebook_download_badge\"></a>\n",
    "\n",
    "<script>\n",
    "var path = window.location.pathname;\n",
    "var pageName = path.split(\"/\").pop().split(\".\")[0];\n",
    "var downloadLink = [\"/\", \"_downloads/ipynb/py/\", pageName, \".ipynb\"].join(\"\");\n",
    "document.getElementById('notebook_download_badge').parentElement.setAttribute('href', downloadLink);\n",
    "</script>\n",
    "\n",
    "<a href=\"/status.html\"><img src=\"https://img.shields.io/badge/Execution%20test-not%20available-lightgrey.svg\" id=\"executability_status_badge\"></a>\n",
    "\n",
    "<div class=\"how-to\">\n",
    "        <a href=\"#\" class=\"toggle\"><span class=\"icon icon-angle-double-down\"></span>How to read this lecture...</a>\n",
    "        <div class=\"how-to-content\">\n",
    "                <p>Code should execute sequentially if run in a Jupyter notebook</p>\n",
    "                <ul>\n",
    "                        <li>See the <a href=\"/py/getting_started.html\">set up page</a> to install Jupyter, Python and all necessary libraries</li>\n",
    "                        <li>Please direct feedback to <a href=\"mailto:contact@quantecon.org\">contact@quantecon.org</a> or the <a href=\"http://discourse.quantecon.org/\">discourse forum</a></li>\n",
    "                </ul>\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Search IV: Modeling Career Choice\n",
    "\n",
    "\n",
    "<a id='index-0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Job Search IV: Modeling Career Choice](#Job-Search-IV:-Modeling-Career-Choice)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Model](#Model)  \n",
    "  - [Implementation: career.py](#Implementation:-career.py)  \n",
    "  - [Exercises](#Exercises)  \n",
    "  - [Solutions](#Solutions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Next we study a computational problem concerning career and job choices\n",
    "\n",
    "The model is originally due to Derek Neal [[Nea99]](zreferences.ipynb#neal1999)\n",
    "\n",
    "This exposition draws on the presentation in [[LS18]](zreferences.ipynb#ljungqvist2012), section 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model features\n",
    "\n",
    "- Career and job within career both chosen to maximize expected discounted wage flow  \n",
    "- Infinite horizon dynamic programming with two state variables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In what follows we distinguish between a career and a job, where\n",
    "\n",
    "- a *career* is understood to be a general field encompassing many possible jobs, and  \n",
    "- a *job*  is understood to be a position with a particular firm  \n",
    "\n",
    "\n",
    "For workers, wages can be decomposed into the contribution of job and career\n",
    "\n",
    "- $ w_t = \\theta_t + \\epsilon_t $, where  \n",
    "  \n",
    "       is contribution of career at time \n",
    "  \n",
    "   is contribution of job at time - $ \\theta_t $$ t $  \n",
    "  - $ \\epsilon_t $$ t $  \n",
    "\n",
    "\n",
    "At the start of time $ t $, a worker has the following options\n",
    "\n",
    "- retain a current (career, job) pair $ (\\theta_t, \\epsilon_t) $\n",
    "  — referred to hereafter as “stay put”  \n",
    "- retain a current career $ \\theta_t $ but redraw a job $ \\epsilon_t $\n",
    "  — referred to hereafter as “new job”  \n",
    "- redraw both a career $ \\theta_t $ and a job $ \\epsilon_t $\n",
    "  — referred to hereafter as “new life”  \n",
    "\n",
    "\n",
    "Draws of $ \\theta $ and $ \\epsilon $ are independent of each other and\n",
    "past values, with\n",
    "\n",
    "- $ \\theta_t \\sim F $  \n",
    "- $ \\epsilon_t \\sim G $  \n",
    "\n",
    "\n",
    "Notice that the worker does not have the option to retain a job but redraw\n",
    "a career — starting a new career always requires starting a new job\n",
    "\n",
    "A young worker aims to maximize the expected sum of discounted wages\n",
    "\n",
    "\n",
    "<a id='equation-exw'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "\\mathbb{E} \\sum_{t=0}^{\\infty} \\beta^t w_t\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(1)\n",
    "</td></tr></table>\n",
    "\n",
    "subject to the choice restrictions specified above\n",
    "\n",
    "Let $ V(\\theta, \\epsilon) $ denote the value function, which is the\n",
    "maximum of [(1)](#equation-exw) over all feasible (career, job) policies, given the\n",
    "initial state $ (\\theta, \\epsilon) $\n",
    "\n",
    "The value function obeys\n",
    "\n",
    "$$\n",
    "V(\\theta, \\epsilon) = \\max\\{I, II, III\\},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-eyes'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& I = \\theta + \\epsilon + \\beta V(\\theta, \\epsilon) \\\\\n",
    "& II = \\theta + \\int \\epsilon' G(d \\epsilon') + \\beta \\int V(\\theta, \\epsilon') G(d \\epsilon') \\nonumber \\\\\n",
    "& III = \\int \\theta' F(d \\theta') + \\int \\epsilon' G(d \\epsilon') + \\beta \\int \\int V(\\theta', \\epsilon') G(d \\epsilon') F(d \\theta') \\nonumber\n",
    "\\end{aligned}\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(2)\n",
    "</td></tr></table>\n",
    "\n",
    "Evidently $ I $, $ II $ and $ III $ correspond to “stay put”, “new job” and “new life”, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterization\n",
    "\n",
    "As in [[LS18]](zreferences.ipynb#ljungqvist2012), section 6.5, we will focus on a discrete version of the model, parameterized as follows:\n",
    "\n",
    "- both $ \\theta $ and $ \\epsilon $ take values in the set np.linspace(0, B, N) — an even grid of $ N $ points between $ 0 $ and $ B $ inclusive  \n",
    "- $ N = 50 $  \n",
    "- $ B = 5 $  \n",
    "- $ \\beta = 0.95 $  \n",
    "\n",
    "\n",
    "The distributions $ F $ and $ G $ are discrete distributions\n",
    "generating draws from the grid points np.linspace(0, B, N)\n",
    "\n",
    "A very useful family of discrete distributions is the Beta-binomial family,\n",
    "with probability mass function\n",
    "\n",
    "$$\n",
    "p(k \\,|\\, n, a, b)\n",
    "= {n \\choose k} \\frac{B(k + a, n - k + b)}{B(a, b)},\n",
    "\\qquad k = 0, \\ldots, n\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- draw $ q $ from a β distribution with shape parameters $ (a, b) $  \n",
    "- run $ n $ independent binary trials, each with success probability $ q $  \n",
    "- $ p(k \\,|\\, n, a, b) $ is the probability of $ k $ successes in these $ n $ trials  \n",
    "\n",
    "\n",
    "Nice properties:\n",
    "\n",
    "- very flexible class of distributions, including uniform, symmetric unimodal, etc.  \n",
    "- only three parameters  \n",
    "\n",
    "\n",
    "Here’s a figure showing the effect of different shape parameters when $ n=50 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom, beta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_probs(n, a, b):\n",
    "    probs = np.zeros(n+1)\n",
    "    for k in range(n+1):\n",
    "        probs[k] = binom(n, k) * beta(k + a, n - k + b) / beta(a, b)\n",
    "    return probs\n",
    "\n",
    "n = 50\n",
    "a_vals = [0.5, 1, 100]\n",
    "b_vals = [0.5, 1, 100]\n",
    "fig, ax = plt.subplots()\n",
    "for a, b in zip(a_vals, b_vals):\n",
    "    ab_label = f'$a = {a:.1f}$, $b = {b:.1f}$'\n",
    "    ax.plot(list(range(0, n+1)), gen_probs(n, a, b), '-o', label=ab_label)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: career.py\n",
    "\n",
    "The code for solving the DP problem described above is found in [this file](https://github.com/QuantEcon/QuantEcon.lectures.code/blob/master/career/career.py), which is repeated here for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon.distributions import BetaBinomial\n",
    "\n",
    "\n",
    "class CareerWorkerProblem:\n",
    "    \"\"\"\n",
    "    An instance of the class is an object with data on a particular\n",
    "    problem of this type, including probabilites, discount factor and\n",
    "    sample space for the variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    β : scalar(float), optional(default=5.0)\n",
    "        Discount factor\n",
    "    B : scalar(float), optional(default=0.95)\n",
    "        Upper bound of for both ϵ and θ\n",
    "    N : scalar(int), optional(default=50)\n",
    "        Number of possible realizations for both ϵ and θ\n",
    "    F_a : scalar(int or float), optional(default=1)\n",
    "        Parameter `a` from the career distribution\n",
    "    F_b : scalar(int or float), optional(default=1)\n",
    "        Parameter `b` from the career distribution\n",
    "    G_a : scalar(int or float), optional(default=1)\n",
    "        Parameter `a` from the job distribution\n",
    "    G_b : scalar(int or float), optional(default=1)\n",
    "        Parameter `b` from the job distribution\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    β, B, N : see Parameters\n",
    "    θ : array_like(float, ndim=1)\n",
    "        A grid of values from 0 to B\n",
    "    ϵ : array_like(float, ndim=1)\n",
    "        A grid of values from 0 to B\n",
    "    F_probs : array_like(float, ndim=1)\n",
    "        The probabilities of different values for F\n",
    "    G_probs : array_like(float, ndim=1)\n",
    "        The probabilities of different values for G\n",
    "    F_mean : scalar(float)\n",
    "        The mean of the distribution for F\n",
    "    G_mean : scalar(float)\n",
    "        The mean of the distribution for G\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, B=5.0, β=0.95, N=50, F_a=1, F_b=1, G_a=1,\n",
    "                 G_b=1):\n",
    "        self.β, self.N, self.B = β, N, B\n",
    "        self.θ = np.linspace(0, B, N)     # set of θ values\n",
    "        self.ϵ = np.linspace(0, B, N)     # set of ϵ values\n",
    "        self.F_probs = BetaBinomial(N-1, F_a, F_b).pdf()\n",
    "        self.G_probs = BetaBinomial(N-1, G_a, G_b).pdf()\n",
    "        self.F_mean = np.sum(self.θ * self.F_probs)\n",
    "        self.G_mean = np.sum(self.ϵ * self.G_probs)\n",
    "\n",
    "        # Store these parameters for str and repr methods\n",
    "        self._F_a, self._F_b = F_a, F_b\n",
    "        self._G_a, self._G_b = G_a, G_b\n",
    "\n",
    "\n",
    "    def bellman_operator(self, v):\n",
    "        \"\"\"\n",
    "        The Bellman operator for the career / job choice model of Neal.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array_like(float)\n",
    "            A 2D NumPy array representing the value function\n",
    "            Interpretation: :math:`v[i, j] = v(\\theta_i, \\epsilon_j)`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_v : array_like(float)\n",
    "            The updated value function Tv as an array of shape v.shape\n",
    "\n",
    "        \"\"\"\n",
    "        new_v = np.empty(v.shape)\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                # stay put\n",
    "                v1 = self.θ[i] + self.ϵ[j] + self.β * v[i, j]\n",
    "\n",
    "                # new job\n",
    "                v2 = self.θ[i] + self.G_mean + self.β * v[i, :] @ self.G_probs\n",
    "\n",
    "                # new life\n",
    "                v3 = self.G_mean + self.F_mean + self.β * self.F_probs @ v @ self.G_probs\n",
    "                new_v[i, j] = max(v1, v2, v3)\n",
    "        return new_v\n",
    "\n",
    "    def get_greedy(self, v):\n",
    "        \"\"\"\n",
    "        Compute optimal actions taking v as the value function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array_like(float)\n",
    "            A 2D NumPy array representing the value function\n",
    "            Interpretation: :math:`v[i, j] = v(\\theta_i, \\epsilon_j)`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        policy : array_like(float)\n",
    "            A 2D NumPy array, where policy[i, j] is the optimal action\n",
    "            at :math:`(\\theta_i, \\epsilon_j)`.\n",
    "\n",
    "            The optimal action is represented as an integer in the set\n",
    "            1, 2, 3, where 1 = 'stay put', 2 = 'new job' and 3 = 'new\n",
    "            life'\n",
    "\n",
    "        \"\"\"\n",
    "        policy = np.empty(v.shape, dtype=int)\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                v1 = self.θ[i] + self.ϵ[j] + self.β * v[i, j]\n",
    "                v2 = self.θ[i] + self.G_mean + self.β * v[i, :] @ self.G_probs\n",
    "                v3 = self.G_mean + self.F_mean + self.β * self.F_probs @ v @ self.G_probs\n",
    "                if v1 > max(v2, v3):\n",
    "                    action = 1\n",
    "                elif v2 > max(v1, v3):\n",
    "                    action = 2\n",
    "                else:\n",
    "                    action = 3\n",
    "                policy[i, j] = action\n",
    "\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code defines\n",
    "\n",
    "- a class CareerWorkerProblem that  \n",
    "  \n",
    "      encapsulates all the details of a particular parameterization\n",
    "  \n",
    "  implements the Bellman operator -   \n",
    "  - $ T $  \n",
    "\n",
    "\n",
    "In this model, $ T $ is defined by $ Tv(\\theta, \\epsilon) = \\max\\{I, II, III\\} $, where\n",
    "$ I $, $ II $ and $ III $ are as given in [(2)](#equation-eyes), replacing $ V $ with $ v $\n",
    "\n",
    "The default probability distributions in CareerWorkerProblem correspond to discrete uniform distributions (see the Beta-binomial figure)\n",
    "\n",
    "In fact all our default settings correspond to the version studied in [[LS18]](zreferences.ipynb#ljungqvist2012), section 6.5.\n",
    "\n",
    "Hence we can reproduce figures 6.5.1 and 6.5.2 shown there, which exhibit the\n",
    "value function and optimal policy respectively\n",
    "\n",
    "Here’s the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import quantecon as qe\n",
    "\n",
    "# === set matplotlib parameters === #\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "\n",
    "# === solve for the value function === #\n",
    "wp = CareerWorkerProblem()\n",
    "v_init = np.ones((wp.N, wp.N)) * 100\n",
    "v = qe.compute_fixed_point(wp.bellman_operator, v_init,\n",
    "                           max_iter=200, print_skip=25)\n",
    "\n",
    "# === plot value function === #\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "tg, eg = np.meshgrid(wp.θ, wp.ϵ)\n",
    "ax.plot_surface(tg,\n",
    "                eg,\n",
    "                v.T,\n",
    "                rstride=2, cstride=2,\n",
    "                cmap=cm.jet,\n",
    "                alpha=0.5,\n",
    "                linewidth=0.25)\n",
    "ax.set_zlim(150, 200)\n",
    "ax.set_xlabel('θ', fontsize=14)\n",
    "ax.set_ylabel('ϵ', fontsize=14)\n",
    "ax.view_init(ax.elev, 225)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy can be represented as follows (see [Exercise 3](#career-ex3) for code)\n",
    "\n",
    "\n",
    "<a id='career-opt-pol'></a>\n",
    "<img src=\"_static/figures/career_solutions_ex3_py.png\" style=\"width:100%;height:100%\">\n",
    "\n",
    "  \n",
    "Interpretation:\n",
    "\n",
    "- If both job and career are poor or mediocre, the worker will experiment with new job and new career  \n",
    "- If career is sufficiently good, the worker will hold it and experiment with new jobs until a sufficiently good one is found  \n",
    "- If both job and career are good, the worker will stay put  \n",
    "\n",
    "\n",
    "Notice that the worker will always hold on to a sufficiently good career, but not necessarily hold on to even the best paying job\n",
    "\n",
    "The reason is that high lifetime wages require both variables to be large, and\n",
    "the worker cannot change careers without changing jobs\n",
    "\n",
    "- Sometimes a good job must be sacrificed in order to change to a better career  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "<a id='career-ex1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Using the default parameterization in the class CareerWorkerProblem,\n",
    "generate and plot typical sample paths for $ \\theta $ and $ \\epsilon $\n",
    "when the worker follows the optimal policy\n",
    "\n",
    "In particular, modulo randomness, reproduce the following figure (where the horizontal axis represents time)\n",
    "\n",
    "<img src=\"_static/figures/career_solutions_ex1_py.png\" style=\"width:100%;height:100%\">\n",
    "\n",
    "  \n",
    "Hint: To generate the draws from the distributions $ F $ and $ G $, use the class [DiscreteRV](https://github.com/QuantEcon/QuantEcon.py/blob/master/quantecon/discrete_rv.py)\n",
    "\n",
    "\n",
    "<a id='career-ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let’s now consider how long it takes for the worker to settle down to a\n",
    "permanent job, given a starting point of $ (\\theta, \\epsilon) = (0, 0) $\n",
    "\n",
    "In other words, we want to study the distribution of the random variable\n",
    "\n",
    "$$\n",
    "T^* := \\text{the first point in time from which the worker's job no longer changes}\n",
    "$$\n",
    "\n",
    "Evidently, the worker’s job becomes permanent if and only if $ (\\theta_t, \\epsilon_t) $ enters the\n",
    "“stay put” region of $ (\\theta, \\epsilon) $ space\n",
    "\n",
    "Letting $ S $ denote this region, $ T^* $ can be expressed as the\n",
    "first passage time to $ S $ under the optimal policy:\n",
    "\n",
    "$$\n",
    "T^* := \\inf\\{t \\geq 0 \\,|\\, (\\theta_t, \\epsilon_t) \\in S\\}\n",
    "$$\n",
    "\n",
    "Collect 25,000 draws of this random variable and compute the median (which should be about 7)\n",
    "\n",
    "Repeat the exercise with $ \\beta=0.99 $ and interpret the change\n",
    "\n",
    "\n",
    "<a id='career-ex3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "As best you can, reproduce [the figure showing the optimal policy](#career-opt-pol)\n",
    "\n",
    "Hint: The get_greedy() method returns a representation of the optimal\n",
    "policy where values 1, 2 and 3 correspond to “stay put”, “new job” and “new life” respectively.  Use this and contourf from matplotlib.pyplot to produce the different shadings.\n",
    "\n",
    "Now set G_a = G_b = 100 and generate a new figure with these parameters.  Interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon import compute_fixed_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Simulate job / career paths\n",
    "\n",
    "In reading the code, recall that optimal_policy[i, j] = policy at\n",
    "$ (\\theta_i, \\epsilon_j) $ = either 1, 2 or 3; meaning ‘stay put’,\n",
    "‘new job’ and ‘new life’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = CareerWorkerProblem()\n",
    "v_init = np.ones((wp.N, wp.N)) * 100\n",
    "v = compute_fixed_point(wp.bellman_operator, v_init, verbose=False, max_iter=200)\n",
    "optimal_policy = wp.get_greedy(v)\n",
    "F = np.cumsum(wp.F_probs)\n",
    "G = np.cumsum(wp.G_probs)\n",
    "\n",
    "def gen_path(T=20):\n",
    "    i = j = 0\n",
    "    θ_index = []\n",
    "    ϵ_index = []\n",
    "    for t in range(T):\n",
    "        if optimal_policy[i, j] == 1:    # Stay put\n",
    "            pass\n",
    "        elif optimal_policy[i, j] == 2:  # New job\n",
    "            j = int(qe.random.draw(G))\n",
    "        else:                            # New life\n",
    "            i, j  = int(qe.random.draw(F)), int(qe.random.draw(G))\n",
    "        θ_index.append(i)\n",
    "        ϵ_index.append(j)\n",
    "    return wp.θ[θ_index], wp.ϵ[ϵ_index]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for ax in axes:\n",
    "    θ_path, ϵ_path = gen_path()\n",
    "    ax.plot(ϵ_path, label='ϵ')\n",
    "    ax.plot(θ_path, label='θ')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The median for the original parameterization can be computed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = CareerWorkerProblem()\n",
    "v_init = np.ones((wp.N, wp.N)) * 100\n",
    "v = compute_fixed_point(wp.bellman_operator, v_init, max_iter=200, print_skip=25)\n",
    "optimal_policy = wp.get_greedy(v)\n",
    "F = np.cumsum(wp.F_probs)\n",
    "G = np.cumsum(wp.G_probs)\n",
    "\n",
    "def gen_first_passage_time():\n",
    "    t = 0\n",
    "    i = j = 0\n",
    "    while True:\n",
    "        if optimal_policy[i, j] == 1:    # Stay put\n",
    "            return t\n",
    "        elif optimal_policy[i, j] == 2:  # New job\n",
    "            j = int(qe.random.draw(G))\n",
    "        else:                            # New life\n",
    "            i, j  = int(qe.random.draw(F)), int(qe.random.draw(G))\n",
    "        t += 1\n",
    "\n",
    "M = 25000 # Number of samples\n",
    "samples = np.empty(M)\n",
    "for i in range(M):\n",
    "    samples[i] = gen_first_passage_time()\n",
    "print(np.median(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the median with $ \\beta=0.99 $ instead of the default\n",
    "value $ \\beta=0.95 $, replace wp = CareerWorkerProblem() with\n",
    "wp = CareerWorkerProblem(β=0.99) and increase the max_iter=200 in v = compute_fixed_point(...) to max_iter=1000\n",
    "\n",
    "The medians are subject to randomness, but should be about 7 and 14 respectively\n",
    "\n",
    "Not surprisingly, more patient workers will wait longer to settle down to their final job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Here’s the code to reproduce the original figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = CareerWorkerProblem()\n",
    "v_init = np.ones((wp.N, wp.N)) * 100\n",
    "v = compute_fixed_point(wp.bellman_operator, v_init, max_iter=200, print_skip=25)\n",
    "optimal_policy = wp.get_greedy(v)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "tg, eg = np.meshgrid(wp.θ, wp.ϵ)\n",
    "lvls=(0.5, 1.5, 2.5, 3.5)\n",
    "ax.contourf(tg, eg, optimal_policy.T, levels=lvls, cmap=cm.winter, alpha=0.5)\n",
    "ax.contour(tg, eg, optimal_policy.T, colors='k', levels=lvls, linewidths=2)\n",
    "ax.set_xlabel('θ', fontsize=14)\n",
    "ax.set_ylabel('ϵ', fontsize=14)\n",
    "ax.text(1.8, 2.5, 'new life', fontsize=14)\n",
    "ax.text(4.5, 2.5, 'new job', fontsize=14, rotation='vertical')\n",
    "ax.text(4.0, 4.5, 'stay put', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to set G_a = G_b = 100 and generate a new figure with\n",
    "these parameters\n",
    "\n",
    "To do this replace: wp = CareerWorkerProblem() with wp = CareerWorkerProblem(G_a=100, G_b=100)\n",
    "\n",
    "In the new figure, you will see that the region for which the worker\n",
    "will stay put has grown because the distribution for $ \\epsilon $\n",
    "has become more concentrated around the mean, making high-paying jobs\n",
    "less realistic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
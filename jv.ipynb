{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='jv'></a>\n",
    "<a href=\"#\"><img src=\"/_static/img/jupyter-notebook-download-blue.svg\" id=\"notebook_download_badge\"></a>\n",
    "\n",
    "<script>\n",
    "var path = window.location.pathname;\n",
    "var pageName = path.split(\"/\").pop().split(\".\")[0];\n",
    "var downloadLink = [\"/\", \"_downloads/ipynb/py/\", pageName, \".ipynb\"].join(\"\");\n",
    "document.getElementById('notebook_download_badge').parentElement.setAttribute('href', downloadLink);\n",
    "</script>\n",
    "\n",
    "<a href=\"/status.html\"><img src=\"https://img.shields.io/badge/Execution%20test-not%20available-lightgrey.svg\" id=\"executability_status_badge\"></a>\n",
    "\n",
    "<div class=\"how-to\">\n",
    "        <a href=\"#\" class=\"toggle\"><span class=\"icon icon-angle-double-down\"></span>How to read this lecture...</a>\n",
    "        <div class=\"how-to-content\">\n",
    "                <p>Code should execute sequentially if run in a Jupyter notebook</p>\n",
    "                <ul>\n",
    "                        <li>See the <a href=\"/py/getting_started.html\">set up page</a> to install Jupyter, Python and all necessary libraries</li>\n",
    "                        <li>Please direct feedback to <a href=\"mailto:contact@quantecon.org\">contact@quantecon.org</a> or the <a href=\"http://discourse.quantecon.org/\">discourse forum</a></li>\n",
    "                </ul>\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Search V: On-the-Job Search\n",
    "\n",
    "\n",
    "<a id='index-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Job Search V: On-the-Job Search](#Job-Search-V:-On-the-Job-Search)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Model](#Model)  \n",
    "  - [Implementation](#Implementation)  \n",
    "  - [Solving for Policies](#Solving-for-Policies)  \n",
    "  - [Exercises](#Exercises)  \n",
    "  - [Solutions](#Solutions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section we solve a simple on-the-job search model\n",
    "\n",
    "- based on [[LS18]](zreferences.ipynb#ljungqvist2012), exercise 6.18, and [[Jov79]](zreferences.ipynb#jovanovic1979)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model features\n",
    "\n",
    "\n",
    "<a id='index-2'></a>\n",
    "- job-specific human capital accumulation combined with on-the-job search  \n",
    "- infinite horizon dynamic programming with one state variable and two controls  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "<a id='index-3'></a>\n",
    "Let\n",
    "\n",
    "- $ x_t $ denote the time-$ t $ job-specific human capital of a worker employed at a given firm  \n",
    "- $ w_t $ denote current wages  \n",
    "\n",
    "\n",
    "Let $ w_t = x_t(1 - s_t - \\phi_t) $, where\n",
    "\n",
    "- $ \\phi_t $ is investment in job-specific human capital for the current role  \n",
    "- $ s_t $ is search effort, devoted to obtaining new offers from other firms.  \n",
    "\n",
    "\n",
    "For as long as the worker remains in the current job, evolution of\n",
    "$ \\{x_t\\} $ is given by $ x_{t+1} = G(x_t, \\phi_t) $\n",
    "\n",
    "When search effort at $ t $ is $ s_t $, the worker receives a new job\n",
    "offer with probability $ \\pi(s_t) \\in [0, 1] $\n",
    "\n",
    "Value of offer is $ U_{t+1} $, where $ \\{U_t\\} $ is iid with common distribution $ F $\n",
    "\n",
    "Worker has the right to reject the current offer and continue with existing job\n",
    "\n",
    "In particular, $ x_{t+1} = U_{t+1} $ if accepts and $ x_{t+1} = G(x_t, \\phi_t) $ if rejects\n",
    "\n",
    "Letting $ b_{t+1} \\in \\{0,1\\} $ be binary with $ b_{t+1} = 1 $ indicating an offer, we can write\n",
    "\n",
    "\n",
    "<a id='equation-jd'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "x_{t+1}\n",
    "= (1 - b_{t+1}) G(x_t, \\phi_t) + b_{t+1}\n",
    "    \\max \\{ G(x_t, \\phi_t), U_{t+1}\\}\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(1)\n",
    "</td></tr></table>\n",
    "\n",
    "Agent’s objective: maximize expected discounted sum of wages via controls $ \\{s_t\\} $ and $ \\{\\phi_t\\} $\n",
    "\n",
    "Taking the expectation of $ V(x_{t+1}) $ and using [(1)](#equation-jd),\n",
    "the Bellman equation for this problem can be written as\n",
    "\n",
    "\n",
    "<a id='equation-jvbell'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "V(x)\n",
    "= \\max_{s + \\phi \\leq 1}\n",
    "    \\left\\{\n",
    "        x (1 - s - \\phi) + \\beta (1 - \\pi(s)) V[G(x, \\phi)] +\n",
    "        \\beta \\pi(s) \\int V[G(x, \\phi) \\vee u] F(du)\n",
    "     \\right\\}.\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(2)\n",
    "</td></tr></table>\n",
    "\n",
    "Here nonnegativity of $ s $ and $ \\phi $ is understood, while\n",
    "$ a \\vee b := \\max\\{a, b\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterization\n",
    "\n",
    "\n",
    "<a id='index-4'></a>\n",
    "In the implementation below, we will focus on the parameterization\n",
    "\n",
    "$$\n",
    "G(x, \\phi) = A (x \\phi)^{\\alpha},\n",
    "\\quad\n",
    "\\pi(s) = \\sqrt s\n",
    "\\quad \\text{and} \\quad\n",
    "F = \\text{Beta}(2, 2)\n",
    "$$\n",
    "\n",
    "with default parameter values\n",
    "\n",
    "- $ A = 1.4 $  \n",
    "- $ \\alpha = 0.6 $  \n",
    "- $ \\beta = 0.96 $  \n",
    "\n",
    "\n",
    "The Beta(2,2) distribution is supported on $ (0,1) $.  It has a unimodal, symmetric density peaked at 0.5\n",
    "\n",
    "\n",
    "<a id='jvboecalc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-of-the-Envelope Calculations\n",
    "\n",
    "Before we solve the model, let’s make some quick calculations that\n",
    "provide intuition on what the solution should look like\n",
    "\n",
    "To begin, observe that the worker has two instruments to build\n",
    "capital and hence wages:\n",
    "\n",
    "1. invest in capital specific to the current job via $ \\phi $  \n",
    "1. search for a new job with better job-specific capital match via $ s $  \n",
    "\n",
    "\n",
    "Since wages are $ x (1 - s - \\phi) $, marginal cost of investment via either $ \\phi $ or $ s $ is identical\n",
    "\n",
    "Our risk neutral worker should focus on whatever instrument has the highest expected return\n",
    "\n",
    "The relative expected return will depend on $ x $\n",
    "\n",
    "For example, suppose first that $ x = 0.05 $\n",
    "\n",
    "- If $ s=1 $ and $ \\phi = 0 $, then since $ G(x,\\phi) = 0 $,\n",
    "  taking expectations of [(1)](#equation-jd) gives expected next period capital equal to $ \\pi(s) \\mathbb{E} U\n",
    "  = \\mathbb{E} U = 0.5 $  \n",
    "- If $ s=0 $ and $ \\phi=1 $, then next period capital is $ G(x, \\phi) = G(0.05, 1) \\approx 0.23 $  \n",
    "\n",
    "\n",
    "Both rates of return are good, but the return from search is better\n",
    "\n",
    "Next suppose that $ x = 0.4 $\n",
    "\n",
    "- If $ s=1 $ and $ \\phi = 0 $, then expected next period capital is again $ 0.5 $  \n",
    "- If $ s=0 $ and $ \\phi = 1 $, then $ G(x, \\phi) = G(0.4, 1) \\approx 0.8 $  \n",
    "\n",
    "\n",
    "Return from investment via $ \\phi $ dominates expected return from search\n",
    "\n",
    "Combining these observations gives us two informal predictions:\n",
    "\n",
    "1. At any given state $ x $, the two controls $ \\phi $ and $ s $ will function primarily as substitutes — worker will focus on whichever instrument has the higher expected return  \n",
    "1. For sufficiently small $ x $, search will be preferable to investment in job-specific human capital.  For larger $ x $, the reverse will be true  \n",
    "\n",
    "\n",
    "Now let’s turn to implementation, and see if we can match our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "<a id='index-5'></a>\n",
    "The following code solves the DP problem described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import fixed_quad as integrate\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "\n",
    "ϵ = 1e-4  # A small number, used in the optimization routine\n",
    "\n",
    "class JvWorker:\n",
    "    r\"\"\"\n",
    "    A Jovanovic-type model of employment with on-the-job search. The\n",
    "    value function is given by\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        V(x) = \\max_{ϕ, s} w(x, ϕ, s)\n",
    "\n",
    "    for\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        w(x, ϕ, s) := x(1 - ϕ - s)\n",
    "                        + β (1 - π(s)) V(G(x, ϕ))\n",
    "                        + β π(s) E V[ \\max(G(x, ϕ), U)]\n",
    "\n",
    "    Here\n",
    "\n",
    "    * x = human capital\n",
    "    * s = search effort\n",
    "    * :math:`ϕ` = investment in human capital\n",
    "    * :math:`π(s)` = probability of new offer given search level s\n",
    "    * :math:`x(1 - ϕ - s)` = wage\n",
    "    * :math:`G(x, ϕ)` = new human capital when current job retained\n",
    "    * U = RV with distribution F -- new draw of human capital\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : scalar(float), optional(default=1.4)\n",
    "        Parameter in human capital transition function\n",
    "    α : scalar(float), optional(default=0.6)\n",
    "        Parameter in human capital transition function\n",
    "    β : scalar(float), optional(default=0.96)\n",
    "        Discount factor\n",
    "    grid_size : scalar(int), optional(default=50)\n",
    "        Grid size for discretization\n",
    "    G : function, optional(default=lambda x, ϕ: A * (x * ϕ)**α)\n",
    "        Transition function for human captial\n",
    "    π : function, optional(default=sqrt)\n",
    "        Function mapping search effort (:math:`s \\in (0,1)`) to\n",
    "        probability of getting new job offer\n",
    "    F : distribution, optional(default=Beta(2,2))\n",
    "        Distribution from which the value of new job offers is drawn\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    A, α, β : see Parameters\n",
    "    x_grid : array_like(float)\n",
    "        The grid over the human capital\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A=1.4, α=0.6, β=0.96, grid_size=50,\n",
    "                 G=None, π=np.sqrt, F=stats.beta(2, 2)):\n",
    "        self.A, self.α, self.β = A, α, β\n",
    "\n",
    "        # === set defaults for G, π and F === #\n",
    "        self.G = G if G is not None else lambda x, ϕ: A * (x * ϕ)**α\n",
    "        self.π = π\n",
    "        self.F = F\n",
    "\n",
    "        # === Set up grid over the state space for DP === #\n",
    "        # Max of grid is the max of a large quantile value for F and the\n",
    "        # fixed point y = G(y, 1).\n",
    "        grid_max = max(A**(1 / (1 - α)), self.F.ppf(1 - ϵ))\n",
    "        self.x_grid = np.linspace(ϵ, grid_max, grid_size)\n",
    "\n",
    "\n",
    "    def bellman_operator(self, V, brute_force=False, return_policies=False):\n",
    "        \"\"\"\n",
    "        Returns the approximate value function TV by applying the\n",
    "        Bellman operator associated with the model to the function V.\n",
    "\n",
    "        Returns TV, or the V-greedy policies s_policy and ϕ_policy when\n",
    "        return_policies=True.  In the function, the array V is replaced below\n",
    "        with a function Vf that implements linear interpolation over the\n",
    "        points (V(x), x) for x in x_grid.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        V : array_like(float)\n",
    "            Array representing an approximate value function\n",
    "        brute_force : bool, optional(default=False)\n",
    "            Default is False. If the brute_force flag is True, then grid\n",
    "            search is performed at each maximization step.\n",
    "        return_policies : bool, optional(default=False)\n",
    "            Indicates whether to return just the updated value function\n",
    "            TV or both the greedy policy computed from V and TV\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s_policy : array_like(float)\n",
    "            The greedy policy computed from V.  Only returned if\n",
    "            return_policies == True\n",
    "        new_V : array_like(float)\n",
    "            The updated value function Tv, as an array representing the\n",
    "            values TV(x) over x in x_grid.\n",
    "\n",
    "        \"\"\"\n",
    "        # === simplify names, set up arrays, etc. === #\n",
    "        G, π, F, β = self.G, self.π, self.F, self.β\n",
    "        Vf = lambda x: np.interp(x, self.x_grid, V)\n",
    "        N = len(self.x_grid)\n",
    "        new_V, s_policy, ϕ_policy = np.empty(N), np.empty(N), np.empty(N)\n",
    "        a, b = F.ppf(0.005), F.ppf(0.995)           # Quantiles, for integration\n",
    "        c1 = lambda z: 1.0 - sum(z)                 # used to enforce s + ϕ <= 1\n",
    "        c2 = lambda z: z[0] - ϵ                     # used to enforce s >= ϵ\n",
    "        c3 = lambda z: z[1] - ϵ                     # used to enforce ϕ >= ϵ\n",
    "        guess = (0.2, 0.2)\n",
    "        constraints = [{\"type\": \"ineq\", \"fun\": i} for i in [c1, c2, c3]]\n",
    "\n",
    "        # === solve r.h.s. of Bellman equation === #\n",
    "        for i, x in enumerate(self.x_grid):\n",
    "\n",
    "            # === set up objective function === #\n",
    "            def w(z):\n",
    "                s, ϕ = z\n",
    "                h = lambda u: Vf(np.maximum(G(x, ϕ), u)) * F.pdf(u)\n",
    "                integral, err = integrate(h, a, b)\n",
    "                q = π(s) * integral + (1.0 - π(s)) * Vf(G(x, ϕ))\n",
    "                # == minus because we minimize == #\n",
    "                return - x * (1.0 - ϕ - s) - β * q\n",
    "\n",
    "            # === either use SciPy solver === #\n",
    "            if not brute_force:\n",
    "                max_s, max_ϕ = minimize(w, guess, constraints=constraints,\n",
    "                                        options={\"disp\": 0},\n",
    "                                        method=\"COBYLA\")[\"x\"]\n",
    "                max_val = -w((max_s, max_ϕ))\n",
    "\n",
    "            # === or search on a grid === #\n",
    "            else:\n",
    "                search_grid = np.linspace(ϵ, 1.0, 15)\n",
    "                max_val = -1.0\n",
    "                for s in search_grid:\n",
    "                    for ϕ in search_grid:\n",
    "                        current_val = -w((s, ϕ)) if s + ϕ <= 1.0 else -1.0\n",
    "                        if current_val > max_val:\n",
    "                            max_val, max_s, max_ϕ = current_val, s, ϕ\n",
    "\n",
    "            # === store results === #\n",
    "            new_V[i] = max_val\n",
    "            s_policy[i], ϕ_policy[i] = max_s, max_ϕ\n",
    "\n",
    "        if return_policies:\n",
    "            return s_policy, ϕ_policy\n",
    "        else:\n",
    "            return new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is written to be relatively generic—and hence reusable\n",
    "\n",
    "- For example, we use generic $ G(x,\\phi) $ instead of specific $ A (x \\phi)^{\\alpha} $  \n",
    "\n",
    "\n",
    "Regarding the imports\n",
    "\n",
    "- fixed_quad is a simple non-adaptive integration routine  \n",
    "- fmin_slsqp is a minimization routine that permits inequality constraints  \n",
    "\n",
    "\n",
    "Next we build a class called JvWorker that\n",
    "\n",
    "- packages all the parameters and other basic attributes of a given model  \n",
    "- implements the method bellman_operator for value function iteration  \n",
    "\n",
    "\n",
    "The bellman_operator method\n",
    "takes a candidate value function $ V $ and updates it to $ TV $ via\n",
    "\n",
    "$$\n",
    "TV(x)\n",
    "= - \\min_{s + \\phi \\leq 1} w(s, \\phi)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-defw'></a>\n",
    "<table width=100%><tr style='background-color: #FFFFFF !important;'>\n",
    "<td width=10%></td>\n",
    "<td width=80%>\n",
    "$$\n",
    "w(s, \\phi)\n",
    " := - \\left\\{\n",
    "         x (1 - s - \\phi) + \\beta (1 - \\pi(s)) V[G(x, \\phi)] +\n",
    "         \\beta \\pi(s) \\int V[G(x, \\phi) \\vee u] F(du)\n",
    "\\right\\}\n",
    "$$\n",
    "</td><td width=10% style='text-align:center !important;'>\n",
    "(3)\n",
    "</td></tr></table>\n",
    "\n",
    "Here we are minimizing instead of maximizing to fit with SciPy’s optimization routines\n",
    "\n",
    "When we represent $ V $, it will be with a NumPy array V giving values on grid x_grid\n",
    "\n",
    "But to evaluate the right-hand side of [(3)](#equation-defw), we need a function, so\n",
    "we replace the arrays V and x_grid with a function Vf that gives linear\n",
    "interpolation of V on x_grid\n",
    "\n",
    "Hence in the preliminaries of bellman_operator\n",
    "\n",
    "- from the array V we define a linear interpolation Vf of its values  \n",
    "  \n",
    "      c1 is used to implement the constraint \n",
    "  \n",
    "  c2 is used to implement , a numerically stable\n",
    "  \n",
    "  alternative to the true constraint \n",
    "  \n",
    "  c3 does the same for - $ s + \\phi \\leq 1 $  \n",
    "  - $ s \\geq \\epsilon $  \n",
    "    $ s \\geq 0 $  \n",
    "  - $ \\phi $  \n",
    "\n",
    "\n",
    "Inside the for loop, for each x in the grid over the state space, we\n",
    "set up the function $ w(z) = w(s, \\phi) $ defined in [(3)](#equation-defw).\n",
    "\n",
    "The function is minimized over all feasible $ (s, \\phi) $ pairs, either by\n",
    "\n",
    "- a relatively sophisticated solver from SciPy called fmin_slsqp, or  \n",
    "- brute force search over a grid  \n",
    "\n",
    "\n",
    "The former is much faster, but convergence to the global optimum is not\n",
    "guaranteed.  Grid search is a simple way to check results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for Policies\n",
    "\n",
    "\n",
    "<a id='index-6'></a>\n",
    "Let’s plot the optimal policies and see what they look like\n",
    "\n",
    "The code is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from quantecon import compute_fixed_point\n",
    "\n",
    "# === solve for optimal policy === #\n",
    "wp = JvWorker(grid_size=25)\n",
    "v_init = wp.x_grid * 0.5\n",
    "V = compute_fixed_point(wp.bellman_operator, v_init, max_iter=40)\n",
    "s_policy, ϕ_policy = wp.bellman_operator(V, return_policies=True)\n",
    "\n",
    "# === plot policies === #\n",
    "\n",
    "plots = [ϕ_policy, s_policy, V]\n",
    "titles = [\"ϕ policy\", \"s policy\", \"value function\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "for ax, plot, title in zip(axes, plots, titles):\n",
    "    ax.plot(wp.x_grid, plot)\n",
    "    ax.set(title=title)\n",
    "    ax.grid()\n",
    "\n",
    "axes[-1].set_xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal axis is the state $ x $, while the vertical axis gives $ s(x) $ and $ \\phi(x) $\n",
    "\n",
    "Overall, the policies match well with our predictions from [section](#jvboecalc)\n",
    "\n",
    "- Worker switches from one investment strategy to the other depending on relative return  \n",
    "- For low values of $ x $, the best option is to search for a new job  \n",
    "- Once $ x $ is larger, worker does better by investing in human capital specific to the current position  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "<a id='jv-ex1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Let’s look at the dynamics for the state process $ \\{x_t\\} $ associated with these policies\n",
    "\n",
    "The dynamics are given by [(1)](#equation-jd) when $ \\phi_t $ and $ s_t $ are\n",
    "chosen according to the optimal policies, and $ \\mathbb{P}\\{b_{t+1} = 1\\}\n",
    "= \\pi(s_t) $\n",
    "\n",
    "Since the dynamics are random, analysis is a bit subtle\n",
    "\n",
    "One way to do it is to plot, for each $ x $ in a relatively fine grid\n",
    "called plot_grid, a\n",
    "large number $ K $ of realizations of $ x_{t+1} $ given $ x_t =\n",
    "x $.  Plot this with one dot for each realization, in the form of a 45 degree\n",
    "diagram.  Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python3\n",
    "K = 50\n",
    "plot_grid_max, plot_grid_size = 1.2, 100\n",
    "plot_grid = np.linspace(0, plot_grid_max, plot_grid_size)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, plot_grid_max)\n",
    "ax.set_ylim(0, plot_grid_max)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the plot, argue that under the optimal policies, the state\n",
    "$ x_t $ will converge to a constant value $ \\bar x $ close to unity\n",
    "\n",
    "Argue that at the steady state, $ s_t \\approx 0 $ and $ \\phi_t \\approx 0.6 $\n",
    "\n",
    "\n",
    "<a id='jv-ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In the preceding exercise we found that $ s_t $ converges to zero\n",
    "and $ \\phi_t $ converges to about 0.6\n",
    "\n",
    "Since these results were calculated at a value of $ \\beta $ close to\n",
    "one, let’s compare them to the best choice for an *infinitely* patient worker\n",
    "\n",
    "Intuitively, an infinitely patient worker would like to maximize steady state\n",
    "wages, which are a function of steady state capital\n",
    "\n",
    "You can take it as given—it’s certainly true—that the infinitely patient worker does not\n",
    "search in the long run (i.e., $ s_t = 0 $ for large $ t $)\n",
    "\n",
    "Thus, given $ \\phi $, steady state capital is the positive fixed point\n",
    "$ x^*(\\phi) $ of the map $ x \\mapsto G(x, \\phi) $\n",
    "\n",
    "Steady state wages can be written as $ w^*(\\phi) = x^*(\\phi) (1 - \\phi) $\n",
    "\n",
    "Graph $ w^*(\\phi) $ with respect to $ \\phi $, and examine the best\n",
    "choice of $ \\phi $\n",
    "\n",
    "Can you give a rough interpretation for the value that you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Here’s code to produce the 45 degree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "wp = JvWorker(grid_size=25)\n",
    "G, π, F = wp.G, wp.π, wp.F       # Simplify names\n",
    "\n",
    "v_init = wp.x_grid * 0.5\n",
    "print(\"Computing value function\")\n",
    "V = compute_fixed_point(wp.bellman_operator, v_init, max_iter=40, verbose=False)\n",
    "print(\"Computing policy functions\")\n",
    "s_policy, ϕ_policy = wp.bellman_operator(V, return_policies=True)\n",
    "\n",
    "# Turn the policy function arrays into actual functions\n",
    "s = lambda y: np.interp(y, wp.x_grid, s_policy)\n",
    "ϕ = lambda y: np.interp(y, wp.x_grid, ϕ_policy)\n",
    "\n",
    "def h(x, b, U):\n",
    "    return (1 - b) * G(x, ϕ(x)) + b * max(G(x, ϕ(x)), U)\n",
    "\n",
    "plot_grid_max, plot_grid_size = 1.2, 100\n",
    "plot_grid = np.linspace(0, plot_grid_max, plot_grid_size)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim(0, plot_grid_max)\n",
    "ax.set_ylim(0, plot_grid_max)\n",
    "ticks = (0.25, 0.5, 0.75, 1.0)\n",
    "ax.set(xticks=ticks, yticks=ticks)\n",
    "ax.set_xlabel('$x_t$', fontsize=16)\n",
    "ax.set_ylabel('$x_{t+1}$', fontsize=16, rotation='horizontal')\n",
    "\n",
    "ax.plot(plot_grid, plot_grid, 'k--')  # 45 degree line\n",
    "for x in plot_grid:\n",
    "    for i in range(50):\n",
    "        b = 1 if random.uniform(0, 1) < π(s(x)) else 0\n",
    "        U = wp.F.rvs(1)\n",
    "        y = h(x, b, U)\n",
    "        ax.plot(x, y, 'go', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dynamics, we can see that\n",
    "\n",
    "- If $ x_t $ is below about 0.2 the dynamics are random, but\n",
    "  $ x_{t+1} > x_t $ is very likely  \n",
    "- As $ x_t $ increases the dynamics become deterministic, and\n",
    "  $ x_t $ converges to a steady state value close to 1  \n",
    "\n",
    "\n",
    "Referring back to the figure here we see that $ x_t \\approx 1 $ means that\n",
    "$ s_t = s(x_t) \\approx 0 $ and\n",
    "$ \\phi_t = \\phi(x_t) \\approx 0.6 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The figure can be produced as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = JvWorker(grid_size=25)\n",
    "\n",
    "def xbar(ϕ):\n",
    "    return (wp.A * ϕ**wp.α)**(1 / (1 - wp.α))\n",
    "\n",
    "ϕ_grid = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "ax.set_xlabel('$\\phi$', fontsize=16)\n",
    "ax.plot(ϕ_grid, [xbar(ϕ) * (1 - ϕ) for ϕ in ϕ_grid], 'b-', label='$w^*(\\phi)$')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the maximizer is around 0.6\n",
    "\n",
    "This this is similar to the long run value for $ \\phi $ obtained in\n",
    "exercise 1\n",
    "\n",
    "Hence the behaviour of the infinitely patent worker is similar to that\n",
    "of the worker with $ \\beta = 0.96 $\n",
    "\n",
    "This seems reasonable, and helps us confirm that our dynamic programming\n",
    "solutions are probably correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}